{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project focuses on developing and assessing machine learning models to classify fault types within a system, leveraging data from a dataset. The data is preprocessed to extract numerical features and encode categorical labels, such as \"Type\", for multi-class classification. Four models are implemented: \n",
    "\n",
    "- Neural Network\n",
    "- Decision Tree\n",
    "- Random Forest with Cross Validation\n",
    "- XGBoost with Grid Search and Cross Validation\n",
    "\n",
    "The project performs both predictive performance and interpretability, employing LIME (Local Interpretable Model-agnostic Explanations) across all models to explain individual predictions and SHAP (SHapley Additive exPlanations) for XGBoost to highlight feature importance. Exploratory data analysis (EDA) and visualizations are used for understanding fault patterns over time. This comprehensive approach underscores the project's goal of accurately diagnosing system faults while ensuring transparency, making it valuable for applications like industrial fault detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, make_scorer, f1_score\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import dtreeviz \n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset and merge the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.csv', parse_dates=[\"Datetime\"])\n",
    "\n",
    "# Shuffle the dataset\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Display the results\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check total number of duplicate rows\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Total number of duplicate rows: {duplicate_count}\")\n",
    "\n",
    "# Show what percentage of the data is duplicated\n",
    "duplicate_percentage = (duplicate_count / len(df)) * 100\n",
    "print(f\"Percentage of duplicates: {duplicate_percentage:.2f}%\")\n",
    "\n",
    "# If you want to see the actual duplicate records\n",
    "if duplicate_count > 0:\n",
    "    print(\"\\nFirst few duplicate records:\")\n",
    "    duplicates = df[df.duplicated(keep='first')]\n",
    "    print(duplicates.head())\n",
    "    \n",
    "    # Count duplicates by how many times each record appears\n",
    "    print(\"\\nDuplicate counts (showing records that appear more than once):\")\n",
    "    duplicate_counts = df.value_counts().reset_index()\n",
    "    duplicate_counts.columns = ['record', 'count']\n",
    "    duplicate_counts = duplicate_counts[duplicate_counts['count'] > 1]\n",
    "    print(duplicate_counts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the input and output variables and split further into training, validation and testing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features and labels\n",
    "X = df.drop('Type', axis=1)\n",
    "y = df['Type']\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Identify numerical columns\n",
    "numerical_cols = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "non_numerical_cols = X_train.select_dtypes(exclude=['float64', 'int64']).columns\n",
    "\n",
    "# Drop non numerical columns\n",
    "X_train = X_train[numerical_cols]\n",
    "X_val = X_val[numerical_cols]\n",
    "X_test = X_test[numerical_cols]\n",
    "\n",
    "# Encode string labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Get the number of classes\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Convert to one-hot encoded format\n",
    "y_train = to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "y_val = to_categorical(y_val_encoded, num_classes=num_classes)\n",
    "y_test = to_categorical(y_test_encoded, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)  # Get predictions\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test_classes, y_pred_classes))\n",
    "print(classification_report(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME for Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A random sample is selected from the dataset to be predicted by every model\n",
    "random_idx = np.random.randint(0, X_test.shape[0])\n",
    "random_sample = X_test.iloc[random_idx:random_idx+1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(X):\n",
    "    return model.predict(X, verbose=0)\n",
    "\n",
    "# Create LIME explainer\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(training_data=X_train.values, feature_names=numerical_cols.tolist(), class_names=list(label_encoder.classes_), mode='classification')\n",
    "prediction = model.predict(random_sample, verbose=0)\n",
    "predicted_class = np.argmax(prediction)\n",
    "print(f\"Random Sample Index: {random_idx}\")\n",
    "print(f\"Predicted Class: {predicted_class} ({label_encoder.classes_[predicted_class]})\")\n",
    "print(f\"Prediction Probabilities: {prediction[0]}\")\n",
    "\n",
    "# Generate LIME explanation\n",
    "lime_exp = explainer.explain_instance(data_row=random_sample.flatten(), predict_fn=predict_proba, num_features=min(10, X_train.shape[1]), top_labels=1)\n",
    "lime_exp_dict = lime_exp.as_list(label=predicted_class)\n",
    "features, contributions = zip(*lime_exp_dict)\n",
    "plt.barh(features, contributions, color=['green' if x > 0 else 'red' for x in contributions])\n",
    "plt.xlabel('Contribution to Prediction')\n",
    "plt.title(f\"LIME Feature Contributions for Predicted Class: {label_encoder.classes_[predicted_class]}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revert one-hot encoded y back to integer labels\n",
    "y_train_int = np.argmax(y_train, axis=1)\n",
    "y_val_int = np.argmax(y_val, axis=1)\n",
    "y_test_int = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Decision Tree Model\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train_int)\n",
    "\n",
    "# Predict on validation set\n",
    "y_val_pred_dt = dt.predict(X_val)\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"Decision Tree Validation Accuracy:\", accuracy_score(y_val_int, y_val_pred_dt))\n",
    "print(\"Decision Tree Validation Classification Report:\")\n",
    "print(classification_report(y_val_int, y_val_pred_dt, target_names=label_encoder.classes_))\n",
    "\n",
    "# Visualize the Decision Tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(dt, feature_names=numerical_cols, class_names=list(label_encoder.classes_), filled=True, rounded=True)\n",
    "plt.title(\"Decision Tree for Fault Type Classification\")\n",
    "plt.show()\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"Decision Tree Test Accuracy:\", accuracy_score(y_test_int, y_test_pred_dt))\n",
    "print(\"Decision Tree Test Classification Report:\")\n",
    "print(classification_report(y_test_int, y_test_pred_dt, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_model = dtreeviz.model(dt, X_train=X_train, y_train=y_train_int, feature_names=numerical_cols, class_names=list(label_encoder.classes_), target_name=\"Fault Type\")\n",
    "viz_model.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME for decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_prediction(model, random_sample, X_train, label_encoder):\n",
    "    prediction = model.predict(random_sample)[0]\n",
    "    predicted_class = label_encoder.classes_[prediction]\n",
    "    print(f\"The model predicted: {predicted_class}\")\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values,feature_names=X_train.columns,class_names=list(label_encoder.classes_),discretize_continuous=True)\n",
    "    exp = explainer.explain_instance(random_sample[0],model.predict_proba,labels=[prediction])\n",
    "    exp.as_pyplot_figure(label=prediction)\n",
    "    plt.title(f\"LIME Explanation for Predicted Class: {predicted_class}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_prediction(dt, random_sample, X_train, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the datasets for cross-validation\n",
    "X = np.vstack((X_train, X_val, X_test))\n",
    "y_int = np.hstack((y_train_int, y_val_int, y_test_int))\n",
    "\n",
    "# Set up Stratified K-Fold Cross-Validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Perform Cross-Validation for Accuracy\n",
    "cv_scores = cross_val_score(rf, X, y_int, cv=cv, scoring='accuracy')\n",
    "print(\"Cross-Validation Accuracy Scores:\", cv_scores)\n",
    "print(\"Mean CV Accuracy:\", np.mean(cv_scores))\n",
    "print(\"Standard Deviation of CV Accuracy:\", np.std(cv_scores))\n",
    "\n",
    "# Cross-Validation for F1-Score (Macro)\n",
    "f1_scorer = make_scorer(f1_score, average='macro')\n",
    "cv_f1_scores = cross_val_score(rf, X, y_int, cv=cv, scoring=f1_scorer)\n",
    "print(\"Cross-Validation F1-Scores:\", cv_f1_scores)\n",
    "print(\"Mean CV F1-Score:\", np.mean(cv_f1_scores))\n",
    "\n",
    "# Train the Model on the Training Set\n",
    "rf.fit(X_train, y_train_int)\n",
    "\n",
    "# Predict and Evaluate on Validation Set\n",
    "y_val_pred_rf = rf.predict(X_val)\n",
    "val_accuracy_rf = accuracy_score(y_val_int, y_val_pred_rf)\n",
    "print(\"Random Forest Validation Accuracy:\", val_accuracy_rf)\n",
    "print(\"Random Forest Validation Classification Report:\")\n",
    "print(classification_report(y_val_int, y_val_pred_rf, target_names=label_encoder.classes_))\n",
    "\n",
    "# Predict and Evaluate on Test Set\n",
    "y_test_pred_rf = rf.predict(X_test)\n",
    "test_accuracy_rf = accuracy_score(y_test_int, y_test_pred_rf)\n",
    "print(\"Random Forest Test Accuracy:\", test_accuracy_rf)\n",
    "print(\"Random Forest Test Classification Report:\")\n",
    "print(classification_report(y_test_int, y_test_pred_rf, target_names=label_encoder.classes_))\n",
    "\n",
    "# Feature Importance\n",
    "importances = rf.feature_importances_\n",
    "feature_names = numerical_cols\n",
    "sorted_idx = np.argsort(importances)[::-1]\n",
    "plt.barh(feature_names[sorted_idx], importances[sorted_idx])\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.title(\"Random Forest Feature Importances\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME for random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_prediction(rf, random_sample, X_train, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost with Grid Search and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBoost classifier\n",
    "xgb = XGBClassifier(eval_metric='mlogloss')     # Multi-class log loss\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],      # Number of trees\n",
    "    'max_depth': [5, 10, 15, 20],            # Maximum tree depth\n",
    "    'learning_rate': [0.1, 0.2],   # Step size shrinkage\n",
    "    'subsample': [0.8, 1.0],             # Fraction of samples per tree\n",
    "    'colsample_bytree': [0.8, 1.0]       # Fraction of features per tree\n",
    "}\n",
    "\n",
    "# Set up Grid Search with Stratified K-Fold\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=cv, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit Grid Search on training data\n",
    "grid_search.fit(X, y_int)\n",
    "\n",
    "# Display the best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Train the best model on the full training set\n",
    "best_xgb = grid_search.best_estimator_\n",
    "best_xgb.fit(X_train, y_train_int)\n",
    "\n",
    "# Feature importance\n",
    "importances = best_xgb.feature_importances_\n",
    "feature_names = numerical_cols\n",
    "sorted_idx = np.argsort(importances)[::-1]\n",
    "plt.barh(feature_names[sorted_idx], importances[sorted_idx])\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.title(\"XGBoost Feature Importances\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain model predictions\n",
    "explainer = shap.Explainer(best_xgb)\n",
    "shap_values = explainer(X_train)\n",
    "shap.summary_plot(shap_values, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_prediction(best_xgb, random_sample, X_train, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
